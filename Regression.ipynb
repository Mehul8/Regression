{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIaYErvMHtjQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regression\n",
        "\n",
        "#Que 1 What is Simple Linear Regression\n",
        "Simple Linear Regression is a method to model the relationship between one independent variable and one dependent variable using a straight line.\n",
        "\n",
        "#Que 2 What are the key assumptions of Simple Linear Regression\n",
        "Key assumptions of Simple Linear Regression:\n",
        "\n",
        "1. Linearity\n",
        "2. Independence of errors\n",
        "3. Homoscedasticity (constant variance of errors)\n",
        "4. Normality of errors\n",
        "\n",
        "#Que 3 What does the coefficient m represent in the equation Y=mX+c\n",
        "The coefficient **m** represents the **slope** of the line — the change in **Y** for a one-unit increase in **X**.\n",
        "\n",
        "#Que 4  What does the intercept c represent in the equation Y=mX+c\n",
        "The intercept **c** is the value of **Y** when **X = 0**; it’s where the line crosses the Y-axis.\n",
        "\n",
        "#Que 5 - How do we calculate the slope m in Simple Linear Regression\n",
        "The slope $m$ is calculated as:\n",
        "\n",
        "$$\n",
        "m = \\frac{n\\sum(xy) - \\sum x \\sum y}{n\\sum(x^2) - (\\sum x)^2}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $n$ = number of data points\n",
        "* $x, y$ = data values of independent and dependent variables\n",
        "\n",
        "#que 6  What is the purpose of the least squares method in Simple Linear Regression\n",
        "The least squares method minimizes the **sum of squared differences** between actual and predicted values to find the best-fitting line.\n",
        "\n",
        "#que 7 How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "R² measures how well the regression line fits the data — it shows the **proportion of variance** in the dependent variable explained by the independent variable.\n",
        "\n",
        "**Range:** 0 to 1\n",
        "**Higher R² = Better fit**\n",
        "\n",
        "#Que 8 What is Multiple Linear Regression\n",
        "Multiple Linear Regression models the relationship between **one dependent variable** and **two or more independent variables** using a linear equation.\n",
        "\n",
        "#Que 9 What is the main difference between Simple and Multiple Linear Regression\n",
        "Simple Linear Regression uses **one** independent variable; Multiple Linear Regression uses **two or more**.\n",
        "\n",
        "#Que 10What are the key assumptions of Multiple Linear Regression\n",
        "\n",
        "Key assumptions of Multiple Linear Regression:\n",
        "\n",
        "1. Linearity\n",
        "2. Independence of errors\n",
        "3. Homoscedasticity\n",
        "4. Normality of errors\n",
        "5. No multicollinearity among independent variables\n",
        "\n",
        "#Que 11- What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "Heteroscedasticity means the **error variance is not constant** across all levels of the independent variables.\n",
        "\n",
        "It can lead to **inefficient estimates** and **invalid hypothesis tests**.\n",
        "\n",
        "#Que 12 How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\n",
        "To reduce high multicollinearity:\n",
        "\n",
        "1. **Remove highly correlated predictors**\n",
        "2. **Use Principal Component Analysis (PCA)**\n",
        "3. **Combine correlated variables**\n",
        "4. **Use Ridge or Lasso regression**\n",
        "\n",
        "#Que 13  What are some common techniques for transforming categorical variables for use in regression models\n",
        "Common techniques:\n",
        "\n",
        "1. **One-Hot Encoding**\n",
        "2. **Label Encoding**\n",
        "3. **Ordinal Encoding**\n",
        "4. **Binary Encoding**\n",
        "5. **Target Encoding**\n",
        "\n",
        "#Que 14  What is the role of interaction terms in Multiple Linear Regression\n",
        "Interaction terms capture the **combined effect** of two or more variables on the dependent variable, beyond their individual effects.\n",
        "\n",
        "#Que 15 - How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "In **Simple Linear Regression**, the intercept is the value of **Y when X = 0**.\n",
        "\n",
        "In **Multiple Linear Regression**, it’s the value of **Y when all independent variables = 0** — which may not always be meaningful.\n",
        "\n",
        "#Que 16 What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "The slope shows **how much Y changes** for a **one-unit change in X**. It directly affects predictions by determining the **direction and strength** of the relationship.\n",
        "\n",
        "#Que 17How does the intercept in a regression model provide context for the relationship between variables\n",
        "The intercept gives the **expected value of Y when all Xs = 0**, helping to **anchor the regression line** and provide a baseline for interpretation.\n",
        "\n",
        "#Que 18- What are the limitations of using R² as a sole measure of model performance\n",
        "Limitations of R²:\n",
        "\n",
        "1. Doesn’t indicate if the model is **appropriate**\n",
        "2. Doesn’t detect **overfitting**\n",
        "3. Doesn’t show **predictive accuracy**\n",
        "4. Can be **misleading** with non-linear data\n",
        "\n",
        "#Que 19 How would you interpret a large standard error for a regression coefficient\n",
        "A large standard error suggests the **coefficient estimate is unstable** and may not be **statistically significant**.\n",
        "\n",
        "#que 20  How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "**Identification:** In residual plots, heteroscedasticity appears as a **funnel shape** or **non-random spread** of residuals.\n",
        "\n",
        "**Importance:** It violates regression assumptions, leading to **biased standard errors** and unreliable **hypothesis tests**.\n",
        "\n",
        "#Que 21 What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "It means the model may include **irrelevant predictors**; high R² is inflated, but adjusted R² corrects for useless variables. This suggests **overfitting**.\n",
        "\n",
        "#Que 22 Why is it important to scale variables in Multiple Linear Regression\n",
        "Scaling ensures all variables are on the **same scale**, improving **interpretability**, **numerical stability**, and performance of **regularized models** like Ridge or Lasso.\n",
        "\n",
        "#Que 23 What is polynomial regression\n",
        "Polynomial regression is a type of regression where the **relationship between variables** is modeled as an **nth-degree polynomial**, allowing for **curved relationships**.\n",
        "\n",
        "#Que 24 How does polynomial regression differ from linear regression\n",
        "Polynomial regression fits a **curved line** (non-linear), while linear regression fits a **straight line**.\n",
        "\n",
        "#Que 25 When is polynomial regression used\n",
        "Polynomial regression is used when data shows a **non-linear** relationship that can be modeled with a **curved trend**.\n",
        "\n",
        "#Que 26 What is the general equation for polynomial regression\u001d\n",
        "The general equation is:\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\dots + \\beta_nx^n + \\varepsilon\n",
        "$$\n",
        "\n",
        "Where $n$ is the degree of the polynomial.\n",
        "\n",
        "#Que 27 Can polynomial regression be applied to multiple variables\n",
        "Yes, polynomial regression can be extended to **multiple variables** by including **interaction** and **power terms** of those variables.\n",
        "\n",
        "#Que 28What are the limitations of polynomial regression\n",
        "Limitations of polynomial regression:\n",
        "\n",
        "1. **Overfitting** with high-degree polynomials\n",
        "2. **Sensitive** to outliers\n",
        "3. **Complex** and hard to interpret\n",
        "4. Poor **extrapolation** beyond data range\n",
        "\n",
        "#Que 29 - What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "Methods to evaluate polynomial degree:\n",
        "\n",
        "1. **Adjusted R²**\n",
        "2. **Cross-validation**\n",
        "3. **AIC/BIC (Information Criteria)**\n",
        "4. **Residual plots**\n",
        "5. **RMSE/MAE** (Error metrics)\n",
        "\n",
        "#Que 30 Why is visualization important in polynomial regression\n",
        "Visualization helps to:\n",
        "\n",
        "1. **Understand data trends**\n",
        "2. **Detect overfitting/underfitting**\n",
        "3. **Interpret model fit**\n",
        "4. Spot **non-linear patterns** clearly\n"
      ],
      "metadata": {
        "id": "hhGILf-3H58m"
      }
    }
  ]
}